"""
LLM Client for OpenAI integration.

This module provides a robust client for interacting with OpenAI's API,
including error handling, retries, and fallback mechanisms.
"""

import asyncio
import logging
from typing import Dict, Any, Optional, List
from dataclasses import dataclass
from enum import Enum
import time

import openai
from openai import AsyncOpenAI
from openai.types.chat import ChatCompletion

from .config import settings

logger = logging.getLogger(__name__)


class ModelType(Enum):
    """Available OpenAI models."""
    GPT_4 = "gpt-4"
    GPT_4_TURBO = "gpt-4-turbo-preview"
    GPT_3_5_TURBO = "gpt-3.5-turbo"


@dataclass
class LLMResponse:
    """Response from LLM with metadata."""
    content: str
    model_used: str
    tokens_used: int
    response_time: float
    success: bool
    error_message: Optional[str] = None


@dataclass
class ConversationMessage:
    """Message in a conversation."""
    role: str  # "system", "user", "assistant"
    content: str
    timestamp: Optional[float] = None


class LLMClient:
    """
    Robust OpenAI client with error handling and fallback mechanisms.
    
    Features:
    - Automatic retries with exponential backoff
    - Model fallback (GPT-4 -> GPT-3.5-turbo)
    - Rate limit handling
    - Comprehensive error logging
    - Token usage tracking
    """
    
    def __init__(self):
        self.client: Optional[AsyncOpenAI] = None
        self.primary_model = settings.OPENAI_MODEL
        self.fallback_model = settings.OPENAI_FALLBACK_MODEL
        self.max_retries = settings.OPENAI_MAX_RETRIES
        self.timeout = settings.OPENAI_TIMEOUT
        self.max_tokens = settings.OPENAI_MAX_TOKENS
        self.temperature = settings.OPENAI_TEMPERATURE
        
    async def initialize(self) -> None:
        """Initialize the OpenAI client."""
        try:
            if not settings.OPENAI_API_KEY:
                logger.warning("OpenAI API key not set. LLM functionality will be limited.")
                return
                
            self.client = AsyncOpenAI(
                api_key=settings.OPENAI_API_KEY,
                timeout=self.timeout
            )
            
            # Test the connection
            await self._test_connection()
            logger.info("LLM client initialized successfully")
            
        except Exception as e:
            logger.error(f"Failed to initialize LLM client: {e}")
            # Don't raise the exception - allow service to start without LLM
            logger.warning("LLM client initialization failed. Service will continue without AI functionality.")
    
    async def cleanup(self) -> None:
        """Cleanup resources."""
        if self.client:
            await self.client.close()
            logger.info("LLM client cleaned up")
    
    async def _test_connection(self) -> None:
        """Test the OpenAI API connection."""
        if not self.client:
            return
            
        try:
            response = await self.client.chat.completions.create(
                model=self.fallback_model,  # Use fallback for testing
                messages=[{"role": "user", "content": "Hello"}],
                max_tokens=10
            )
            logger.info("OpenAI API connection test successful")
        except Exception as e:
            logger.error(f"OpenAI API connection test failed: {e}")
            raise
    
    def _clean_response_formatting(self, content: str) -> str:
        """
        Clean up markdown formatting and make text speech-friendly.
        
        Args:
            content: Raw response content
            
        Returns:
            Cleaned content without markdown formatting
        """
        if not content:
            return content
            
        # Remove markdown bold formatting
        content = content.replace('**', '')
        content = content.replace('__', '')
        
        # Remove markdown italic formatting
        content = content.replace('*', '')
        content = content.replace('_', '')
        
        # Clean up any remaining markdown-style formatting
        import re
        
        # Remove markdown headers
        content = re.sub(r'^#+\s*', '', content, flags=re.MULTILINE)
        
        # Remove markdown links but keep the text
        content = re.sub(r'\[([^\]]+)\]\([^\)]+\)', r'\1', content)
        
        # Remove markdown code blocks
        content = re.sub(r'```[^`]*```', '', content)
        content = re.sub(r'`([^`]+)`', r'\1', content)
        
        # Clean up extra whitespace
        content = re.sub(r'\n\s*\n\s*\n', '\n\n', content)
        content = content.strip()
        
        return content

    async def generate_response(
        self,
        messages: List[ConversationMessage],
        language: str = "en",
        model: Optional[str] = None,
        max_tokens: Optional[int] = None,
        temperature: Optional[float] = None
    ) -> LLMResponse:
        """
        Generate a response using the LLM with knowledge base integration.
        
        Args:
            messages: Conversation history
            language: Target language for response
            model: Specific model to use (optional)
            max_tokens: Maximum tokens for response
            temperature: Response creativity (0.0-1.0)
            
        Returns:
            LLMResponse with generated content and metadata
        """
        start_time = time.time()
        
        # If client is not initialized, return a mock response
        if not self.client:
            return LLMResponse(
                content="AI assistant unavailable. Check OpenAI API configuration.",
                model_used="mock",
                tokens_used=0,
                response_time=time.time() - start_time,
                success=False,
                error_message="OpenAI client not initialized"
            )
        
        # Prepare parameters
        model_to_use = model or self.primary_model
        max_tokens = max_tokens or self.max_tokens
        temperature = temperature or self.temperature
        
        # Get the user's latest message for knowledge base search
        user_message = ""
        if messages:
            # Find the last user message
            for msg in reversed(messages):
                if msg.role == "user":
                    user_message = msg.content
                    break
        
        # Search knowledge base for relevant information
        knowledge_context = ""
        if user_message:
            try:
                # Import here to avoid circular imports
                from .services.knowledge_base import KnowledgeBaseService
                from .services.vector_database import VectorDatabase
                
                # Initialize knowledge base service
                vector_db = VectorDatabase()
                knowledge_service = KnowledgeBaseService(self, vector_db)
                
                # Enhanced search strategy with multiple specific queries
                search_queries = [user_message]
                
                # Add specific search terms for common queries
                if any(word in user_message.lower() for word in ['start', 'startup', 'begin', 'turn on', 'power on', 'how to start']):
                    search_queries.extend([
                        "Section 8 Step 3 Pre-operation Check and Warm-Up",
                        "Turn on master switch PLC set desired cleaning profile depth",
                        "Attach umbilical hose rear Power Pack Assembly",
                        "Lift AutoBoss into water start up via remote control",
                        "Step 3 Pre-operation Check Warm-Up master switch",
                        "operating the AutoBoss Section 8 startup procedure",
                        "Step 1 Turn on master switch Step 2 PLC set desired cleaning",
                        "Step 4 Lift AutoBoss into water start up remote control"
                    ])
                
                # Also search for troubleshooting if user mentions problems
                if any(word in user_message.lower() for word in ['problem', 'issue', 'trouble', 'not working', 'broken', 'error']):
                    search_queries.extend([
                        "troubleshooting guide Section 10",
                        "HP Water Gauge Reading Low",
                        "Walking Wheels slow won't turn",
                        "Remote working intermittently"
                    ])
                
                # Search for HP gauge specific issues
                if any(word in user_message.lower() for word in ['hp', 'pressure', 'gauge', 'red', 'low', 'high']):
                    search_queries.extend([
                        "HP Water Gauge Reading Low in low red zone",
                        "HP Water Gauge Reading High in high red zone", 
                        "charge pressure gauge",
                        "water pressure system",
                        "unloader valve system"
                    ])
                
                # Search for maintenance related queries
                if any(word in user_message.lower() for word in ['maintenance', 'service', 'repair', 'replace', 'check']):
                    search_queries.extend([
                        "Daily Monitoring and Maintenance",
                        "Weekly Monitoring and Maintenance",
                        "250 hour maintenance requirements",
                        "500 hour maintenance requirements",
                        "maintenance check sheet"
                    ])
                
                all_results = []
                for query in search_queries:
                    search_results = await knowledge_service.search_documents(
                        query=query,
                        language=language,
                        limit=5  # Get top 5 results per query
                    )
                    all_results.extend(search_results)
                
                # Remove duplicates and get best results
                seen_docs = set()
                unique_results = []
                for result in all_results:
                    doc_id = result['document']['document_id']
                    chunk_key = f"{doc_id}_{result['matched_content'][:100]}"  # Use content snippet as key
                    if chunk_key not in seen_docs:
                        seen_docs.add(chunk_key)
                        unique_results.append(result)
                
                # Sort by relevance and take top 8 for better coverage
                unique_results.sort(key=lambda x: x['relevance_score'], reverse=True)
                final_results = unique_results[:8]
                
                # Build context from search results with better formatting
                if final_results:
                    context_parts = []
                    for i, result in enumerate(final_results):
                        doc = result['document']
                        content = result['matched_content']
                        # Include more content for better context
                        if len(content) > 1500:
                            content = content[:1500] + "..."
                        
                        # Add machine model info if available
                        model_info = ""
                        if doc.get('machine_models'):
                            model_info = f" (AutoBoss {', '.join(doc['machine_models'])})"
                        
                        context_parts.append(f"=== MANUAL SECTION {i+1} ===\nFrom: {doc['title']}{model_info}\nRelevance: {result['relevance_score']:.3f}\nContent: {content}")
                    
                    knowledge_context = "\n\n".join(context_parts)
                    logger.info(f"Found {len(final_results)} relevant knowledge base entries")
                    logger.debug(f"Knowledge context length: {len(knowledge_context)} characters")
                    logger.debug(f"Search queries used: {search_queries}")
                
            except Exception as e:
                logger.warning(f"Failed to search knowledge base: {e}")
                # Continue without knowledge base context
        
        # Convert messages to OpenAI format
        openai_messages = [
            {"role": msg.role, "content": msg.content}
            for msg in messages
        ]
        
        # Build system message based on whether we have knowledge context
        system_content = ""
        
        if knowledge_context:
            # Create a concise system prompt with proper formatting
            system_content = f"""You are AutoBoss AI Assistant for AutoBoss net cleaning machines.

MANUAL CONTENT:
{knowledge_context}

INSTRUCTIONS:
1. Use ONLY the manual content above
2. Be direct and concise - avoid phrases like "I'm sorry to hear that"
3. Provide step-by-step instructions when needed
4. Include section references when available
5. Prioritize safety
6. Use plain text formatting - NO markdown asterisks or bold formatting
7. For emphasis, use CAPITAL LETTERS or numbered lists
8. Respond in {language}

RESPONSE STYLE:
- Direct and to-the-point
- Clear numbered steps
- No unnecessary pleasantries
- Technical but accessible language
- Include specific values and readings from manual"""
            logger.info(f"Using comprehensive system prompt with {len(knowledge_context)} characters of manual content")
        else:
            # No knowledge context - use concise basic system prompt
            system_content = f"""You are AutoBoss AI Assistant for AutoBoss net cleaning machines.

Be direct and concise. Avoid phrases like "I'm sorry to hear that". Use plain text - NO markdown formatting or asterisks. 

If you don't have specific AutoBoss information, state this clearly and suggest contacting Oraseas support.

Respond in {language}."""
            logger.info("Using basic system prompt - no knowledge context found")
        
        openai_messages.insert(0, {
            "role": "system",
            "content": system_content
        })
        
        # Attempt generation with retries and fallback
        for attempt in range(self.max_retries):
            try:
                response = await self._make_api_call(
                    messages=openai_messages,
                    model=model_to_use,
                    max_tokens=max_tokens,
                    temperature=temperature
                )
                
                response_time = time.time() - start_time
                
                # Clean the response content to remove markdown formatting
                cleaned_content = self._clean_response_formatting(response.choices[0].message.content)
                
                return LLMResponse(
                    content=cleaned_content,
                    model_used=response.model,
                    tokens_used=response.usage.total_tokens,
                    response_time=response_time,
                    success=True
                )
                
            except openai.RateLimitError as e:
                logger.warning(f"Rate limit hit on attempt {attempt + 1}: {e}")
                if attempt < self.max_retries - 1:
                    await asyncio.sleep(2 ** attempt)  # Exponential backoff
                    continue
                else:
                    return self._create_error_response(
                        f"Rate limit exceeded after {self.max_retries} attempts",
                        time.time() - start_time
                    )
                    
            except openai.APIError as e:
                logger.error(f"OpenAI API error on attempt {attempt + 1}: {e}")
                
                # Try fallback model if using primary model
                if model_to_use == self.primary_model and attempt == 0:
                    logger.info("Falling back to secondary model")
                    model_to_use = self.fallback_model
                    continue
                    
                if attempt < self.max_retries - 1:
                    await asyncio.sleep(2 ** attempt)
                    continue
                else:
                    return self._create_error_response(
                        f"API error after {self.max_retries} attempts: {str(e)}",
                        time.time() - start_time
                    )
                    
            except Exception as e:
                logger.error(f"Unexpected error on attempt {attempt + 1}: {e}")
                if attempt < self.max_retries - 1:
                    await asyncio.sleep(2 ** attempt)
                    continue
                else:
                    return self._create_error_response(
                        f"Unexpected error after {self.max_retries} attempts: {str(e)}",
                        time.time() - start_time
                    )
        
        # Should not reach here, but just in case
        return self._create_error_response(
            "Failed to generate response after all retries",
            time.time() - start_time
        )
    
    async def _make_api_call(
        self,
        messages: List[Dict[str, str]],
        model: str,
        max_tokens: int,
        temperature: float
    ) -> ChatCompletion:
        """Make the actual API call to OpenAI."""
        return await self.client.chat.completions.create(
            model=model,
            messages=messages,
            max_tokens=max_tokens,
            temperature=temperature,
            timeout=self.timeout
        )
    
    def _get_language_instruction(self, language: str) -> str:
        """Get language-specific instruction for the LLM."""
        language_instructions = {
            "en": {
                "instruction": "Respond in English",
                "context": "You are an expert AutoBoss machine troubleshooting assistant. Provide clear, step-by-step guidance for diagnosing and resolving issues with AutoBoss net cleaning machines. Use technical terminology appropriately and always prioritize safety."
            },
            "el": {
                "instruction": "Î‘Ï€Î±Î½Ï„Î®ÏƒÏ„Îµ ÏƒÏ„Î± Î•Î»Î»Î·Î½Î¹ÎºÎ¬ (Greek)",
                "context": "Î•Î¯ÏƒÏ„Îµ ÎµÎ¹Î´Î¹ÎºÏŒÏ‚ Î²Î¿Î·Î¸ÏŒÏ‚ Î±Î½Ï„Î¹Î¼ÎµÏ„ÏŽÏ€Î¹ÏƒÎ·Ï‚ Ï€ÏÎ¿Î²Î»Î·Î¼Î¬Ï„Ï‰Î½ Î¼Î·Ï‡Î±Î½Î·Î¼Î¬Ï„Ï‰Î½ AutoBoss. Î Î±ÏÎ­Ï‡ÎµÏ„Îµ ÏƒÎ±Ï†ÎµÎ¯Ï‚, Î²Î®Î¼Î± Ï€ÏÎ¿Ï‚ Î²Î®Î¼Î± Î¿Î´Î·Î³Î¯ÎµÏ‚ Î³Î¹Î± Ï„Î· Î´Î¹Î¬Î³Î½Ï‰ÏƒÎ· ÎºÎ±Î¹ Ï„Î·Î½ ÎµÏ€Î¯Î»Ï…ÏƒÎ· Ï€ÏÎ¿Î²Î»Î·Î¼Î¬Ï„Ï‰Î½ Î¼Îµ Ï„Î± Î¼Î·Ï‡Î±Î½Î®Î¼Î±Ï„Î± ÎºÎ±Î¸Î±ÏÎ¹ÏƒÎ¼Î¿Ï Î´Î¹ÎºÏ„ÏÏ‰Î½ AutoBoss. Î§ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î®ÏƒÏ„Îµ Ï„Î·Î½ Ï„ÎµÏ‡Î½Î¹ÎºÎ® Î¿ÏÎ¿Î»Î¿Î³Î¯Î± ÎºÎ±Ï„Î¬Î»Î»Î·Î»Î± ÎºÎ±Î¹ Î´ÏŽÏƒÏ„Îµ Ï€Î¬Î½Ï„Î± Ï€ÏÎ¿Ï„ÎµÏÎ±Î¹ÏŒÏ„Î·Ï„Î± ÏƒÏ„Î·Î½ Î±ÏƒÏ†Î¬Î»ÎµÎ¹Î±."
            },
            "ar": {
                "instruction": "Ø£Ø¬Ø¨ Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© (Arabic)",
                "context": "Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ø®Ø¨ÙŠØ± ÙÙŠ Ø§Ø³ØªÙƒØ´Ø§Ù Ø£Ø®Ø·Ø§Ø¡ Ø¢Ù„Ø§Øª AutoBoss ÙˆØ¥ØµÙ„Ø§Ø­Ù‡Ø§. Ù‚Ø¯Ù… Ø¥Ø±Ø´Ø§Ø¯Ø§Øª ÙˆØ§Ø¶Ø­Ø© Ø®Ø·ÙˆØ© Ø¨Ø®Ø·ÙˆØ© Ù„ØªØ´Ø®ÙŠØµ ÙˆØ­Ù„ Ø§Ù„Ù…Ø´Ø§ÙƒÙ„ Ù…Ø¹ Ø¢Ù„Ø§Øª ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø´Ø¨Ø§Ùƒ AutoBoss. Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù…ØµØ·Ù„Ø­Ø§Øª Ø§Ù„ØªÙ‚Ù†ÙŠØ© Ø¨Ø´ÙƒÙ„ Ù…Ù†Ø§Ø³Ø¨ ÙˆØ£Ø¹Ø· Ø§Ù„Ø£ÙˆÙ„ÙˆÙŠØ© Ø¯Ø§Ø¦Ù…Ø§Ù‹ Ù„Ù„Ø³Ù„Ø§Ù…Ø©."
            },
            "es": {
                "instruction": "Responde en EspaÃ±ol (Spanish)",
                "context": "Eres un asistente experto en soluciÃ³n de problemas de mÃ¡quinas AutoBoss. Proporciona orientaciÃ³n clara, paso a paso, para diagnosticar y resolver problemas con las mÃ¡quinas de limpieza de redes AutoBoss. Usa la terminologÃ­a tÃ©cnica apropiadamente y siempre prioriza la seguridad."
            },
            "tr": {
                "instruction": "TÃ¼rkÃ§e yanÄ±tlayÄ±n (Turkish)",
                "context": "AutoBoss makine sorun giderme konusunda uzman bir asistansÄ±nÄ±z. AutoBoss aÄŸ temizleme makinelerindeki sorunlarÄ± teÅŸhis etmek ve Ã§Ã¶zmek iÃ§in aÃ§Ä±k, adÄ±m adÄ±m rehberlik saÄŸlayÄ±n. Teknik terminolojiyi uygun ÅŸekilde kullanÄ±n ve her zaman gÃ¼venliÄŸi Ã¶nceliklendirin."
            },
            "no": {
                "instruction": "Svar pÃ¥ Norsk (Norwegian)",
                "context": "Du er en ekspert AutoBoss maskin feilsÃ¸kingsassistent. Gi klar, trinn-for-trinn veiledning for Ã¥ diagnostisere og lÃ¸se problemer med AutoBoss nettrengjÃ¸ringsmaskiner. Bruk teknisk terminologi pÃ¥ riktig mÃ¥te og prioriter alltid sikkerhet."
            }
        }
        
        lang_config = language_instructions.get(language, language_instructions["en"])
        return f"{lang_config['instruction']}. {lang_config['context']}"
    
    def _create_error_response(self, error_message: str, response_time: float) -> LLMResponse:
        """Create an error response."""
        return LLMResponse(
            content="AI assistant temporarily unavailable. Please try again.",
            model_used="error",
            tokens_used=0,
            response_time=response_time,
            success=False,
            error_message=error_message
        )
    
    async def analyze_problem(
        self,
        problem_description: str,
        machine_context: Optional[Dict[str, Any]] = None,
        language: str = "en"
    ) -> LLMResponse:
        """
        Analyze a problem description and provide initial diagnostic assessment.
        
        Args:
            problem_description: User's description of the problem
            machine_context: Information about the specific machine
            language: Language for the response
            
        Returns:
            LLMResponse with diagnostic analysis
        """
        # Build system prompt
        system_prompt = self._build_diagnostic_system_prompt(machine_context, language)
        
        # Build user prompt
        user_prompt = f"Problem description: {problem_description}\n\nPlease provide an initial diagnostic assessment and suggest troubleshooting steps."
        
        messages = [
            ConversationMessage(role="system", content=system_prompt),
            ConversationMessage(role="user", content=user_prompt)
        ]
        
        return await self.generate_response(messages, language=language)
    
    def _build_diagnostic_system_prompt(self, machine_context: Optional[Dict[str, Any]] = None, language: str = "en") -> str:
        """Build system prompt for diagnostic analysis."""
        
        # Language-specific base prompts
        base_prompts = {
            "en": """You are an expert AutoBoss machine troubleshooting assistant. Your role is to help operators diagnose and resolve issues with their AutoBoss net cleaning machines.

When analyzing problems:
1. Ask clarifying questions if the problem description is unclear
2. Provide step-by-step diagnostic procedures
3. Suggest the most likely causes first
4. Include safety considerations
5. Recommend when to escalate to expert support

Always be helpful, clear, and safety-conscious in your responses.""",
            
            "el": """Î•Î¯ÏƒÏ„Îµ ÎµÎ¹Î´Î¹ÎºÏŒÏ‚ Î²Î¿Î·Î¸ÏŒÏ‚ Î±Î½Ï„Î¹Î¼ÎµÏ„ÏŽÏ€Î¹ÏƒÎ·Ï‚ Ï€ÏÎ¿Î²Î»Î·Î¼Î¬Ï„Ï‰Î½ Î¼Î·Ï‡Î±Î½Î·Î¼Î¬Ï„Ï‰Î½ AutoBoss. ÎŸ ÏÏŒÎ»Î¿Ï‚ ÏƒÎ±Ï‚ ÎµÎ¯Î½Î±Î¹ Î½Î± Î²Î¿Î·Î¸Î®ÏƒÎµÏ„Îµ Ï„Î¿Ï…Ï‚ Ï‡ÎµÎ¹ÏÎ¹ÏƒÏ„Î­Ï‚ Î½Î± Î´Î¹Î±Î³Î½ÏŽÏƒÎ¿Ï…Î½ ÎºÎ±Î¹ Î½Î± ÎµÏ€Î¹Î»ÏÏƒÎ¿Ï…Î½ Ï€ÏÎ¿Î²Î»Î®Î¼Î±Ï„Î± Î¼Îµ Ï„Î± Î¼Î·Ï‡Î±Î½Î®Î¼Î±Ï„Î± ÎºÎ±Î¸Î±ÏÎ¹ÏƒÎ¼Î¿Ï Î´Î¹ÎºÏ„ÏÏ‰Î½ AutoBoss.

ÎšÎ±Ï„Î¬ Ï„Î·Î½ Î±Î½Î¬Î»Ï…ÏƒÎ· Ï€ÏÎ¿Î²Î»Î·Î¼Î¬Ï„Ï‰Î½:
1. ÎšÎ¬Î½Ï„Îµ Î´Î¹ÎµÏ…ÎºÏÎ¹Î½Î¹ÏƒÏ„Î¹ÎºÎ­Ï‚ ÎµÏÏ‰Ï„Î®ÏƒÎµÎ¹Ï‚ ÎµÎ¬Î½ Î· Ï€ÎµÏÎ¹Î³ÏÎ±Ï†Î® Ï„Î¿Ï… Ï€ÏÎ¿Î²Î»Î®Î¼Î±Ï„Î¿Ï‚ Î´ÎµÎ½ ÎµÎ¯Î½Î±Î¹ ÏƒÎ±Ï†Î®Ï‚
2. Î Î±ÏÎ­Ï‡ÎµÏ„Îµ Î²Î®Î¼Î± Ï€ÏÎ¿Ï‚ Î²Î®Î¼Î± Î´Î¹Î±Î³Î½Ï‰ÏƒÏ„Î¹ÎºÎ­Ï‚ Î´Î¹Î±Î´Î¹ÎºÎ±ÏƒÎ¯ÎµÏ‚
3. Î ÏÎ¿Ï„ÎµÎ¯Î½ÎµÏ„Îµ Ï€ÏÏŽÏ„Î± Ï„Î¹Ï‚ Ï€Î¹Î¿ Ï€Î¹Î¸Î±Î½Î­Ï‚ Î±Î¹Ï„Î¯ÎµÏ‚
4. Î£Ï…Î¼Ï€ÎµÏÎ¹Î»Î¬Î²ÎµÏ„Îµ Î¸Î­Î¼Î±Ï„Î± Î±ÏƒÏ†Î±Î»ÎµÎ¯Î±Ï‚
5. Î£Ï…ÏƒÏ„Î®ÏƒÏ„Îµ Ï€ÏŒÏ„Îµ Î½Î± ÎºÎ»Î¹Î¼Î±ÎºÏŽÏƒÎµÏ„Îµ ÏƒÎµ ÎµÎ¹Î´Î¹ÎºÎ® Ï…Ï€Î¿ÏƒÏ„Î®ÏÎ¹Î¾Î·

ÎÎ± ÎµÎ¯ÏƒÏ„Îµ Ï€Î¬Î½Ï„Î± Ï‡ÏÎ®ÏƒÎ¹Î¼Î¿Î¹, ÏƒÎ±Ï†ÎµÎ¯Ï‚ ÎºÎ±Î¹ Ï€ÏÎ¿ÏƒÎµÎºÏ„Î¹ÎºÎ¿Î¯ ÏƒÏ„Î·Î½ Î±ÏƒÏ†Î¬Î»ÎµÎ¹Î± ÏƒÏ„Î¹Ï‚ Î±Ï€Î±Î½Ï„Î®ÏƒÎµÎ¹Ï‚ ÏƒÎ±Ï‚.""",
            
            "ar": """Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ø®Ø¨ÙŠØ± ÙÙŠ Ø§Ø³ØªÙƒØ´Ø§Ù Ø£Ø®Ø·Ø§Ø¡ Ø¢Ù„Ø§Øª AutoBoss ÙˆØ¥ØµÙ„Ø§Ø­Ù‡Ø§. Ø¯ÙˆØ±Ùƒ Ù‡Ùˆ Ù…Ø³Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø´ØºÙ„ÙŠÙ† ÙÙŠ ØªØ´Ø®ÙŠØµ ÙˆØ­Ù„ Ø§Ù„Ù…Ø´Ø§ÙƒÙ„ Ù…Ø¹ Ø¢Ù„Ø§Øª ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø´Ø¨Ø§Ùƒ AutoBoss.

Ø¹Ù†Ø¯ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø§ÙƒÙ„:
1. Ø§Ø·Ø±Ø­ Ø£Ø³Ø¦Ù„Ø© ØªÙˆØ¶ÙŠØ­ÙŠØ© Ø¥Ø°Ø§ ÙƒØ§Ù† ÙˆØµÙ Ø§Ù„Ù…Ø´ÙƒÙ„Ø© ØºÙŠØ± ÙˆØ§Ø¶Ø­
2. Ù‚Ø¯Ù… Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª ØªØ´Ø®ÙŠØµÙŠØ© Ø®Ø·ÙˆØ© Ø¨Ø®Ø·ÙˆØ©
3. Ø§Ù‚ØªØ±Ø­ Ø§Ù„Ø£Ø³Ø¨Ø§Ø¨ Ø§Ù„Ø£ÙƒØ«Ø± Ø§Ø­ØªÙ…Ø§Ù„Ø§Ù‹ Ø£ÙˆÙ„Ø§Ù‹
4. Ø§Ø´Ù…Ù„ Ø§Ø¹ØªØ¨Ø§Ø±Ø§Øª Ø§Ù„Ø³Ù„Ø§Ù…Ø©
5. Ø£ÙˆØµÙ Ø¨Ù…ÙˆØ¹Ø¯ Ø§Ù„ØªØµØ¹ÙŠØ¯ Ø¥Ù„Ù‰ Ø§Ù„Ø¯Ø¹Ù… Ø§Ù„Ù…ØªØ®ØµØµ

ÙƒÙ† Ø¯Ø§Ø¦Ù…Ø§Ù‹ Ù…ÙÙŠØ¯Ø§Ù‹ ÙˆÙˆØ§Ø¶Ø­Ø§Ù‹ ÙˆÙ…Ù‡ØªÙ…Ø§Ù‹ Ø¨Ø§Ù„Ø³Ù„Ø§Ù…Ø© ÙÙŠ Ø±Ø¯ÙˆØ¯Ùƒ.""",
            
            "es": """Eres un asistente experto en soluciÃ³n de problemas de mÃ¡quinas AutoBoss. Tu funciÃ³n es ayudar a los operadores a diagnosticar y resolver problemas con sus mÃ¡quinas de limpieza de redes AutoBoss.

Al analizar problemas:
1. Haz preguntas aclaratorias si la descripciÃ³n del problema no estÃ¡ clara
2. Proporciona procedimientos de diagnÃ³stico paso a paso
3. Sugiere las causas mÃ¡s probables primero
4. Incluye consideraciones de seguridad
5. Recomienda cuÃ¡ndo escalar al soporte experto

SÃ© siempre Ãºtil, claro y consciente de la seguridad en tus respuestas.""",
            
            "tr": """AutoBoss makine sorun giderme konusunda uzman bir asistansÄ±nÄ±z. RolÃ¼nÃ¼z, operatÃ¶rlerin AutoBoss aÄŸ temizleme makinelerindeki sorunlarÄ± teÅŸhis etmelerine ve Ã§Ã¶zmelerine yardÄ±mcÄ± olmaktÄ±r.

SorunlarÄ± analiz ederken:
1. Sorun aÃ§Ä±klamasÄ± belirsizse aÃ§Ä±klayÄ±cÄ± sorular sorun
2. AdÄ±m adÄ±m tanÄ± prosedÃ¼rleri saÄŸlayÄ±n
3. En olasÄ± nedenleri Ã¶nce Ã¶nerin
4. GÃ¼venlik hususlarÄ±nÄ± dahil edin
5. Uzman desteÄŸe ne zaman yÃ¼kseltileceÄŸini tavsiye edin

YanÄ±tlarÄ±nÄ±zda her zaman yardÄ±mcÄ±, aÃ§Ä±k ve gÃ¼venlik bilincinde olun.""",
            
            "no": """Du er en ekspert AutoBoss maskin feilsÃ¸kingsassistent. Din rolle er Ã¥ hjelpe operatÃ¸rer med Ã¥ diagnostisere og lÃ¸se problemer med deres AutoBoss nettrengjÃ¸ringsmaskiner.

NÃ¥r du analyserer problemer:
1. Still oppklarende spÃ¸rsmÃ¥l hvis problembeskrivelsen er uklar
2. Gi trinn-for-trinn diagnostiske prosedyrer
3. ForeslÃ¥ de mest sannsynlige Ã¥rsakene fÃ¸rst
4. Inkluder sikkerhetshensyn
5. Anbefal nÃ¥r man skal eskalere til ekspertstÃ¸tte

VÃ¦r alltid hjelpsom, klar og sikkerhetsbevisst i dine svar."""
        }

        base_prompt = base_prompts.get(language, base_prompts["en"])

        if machine_context:
            # Extract machine details
            machine_details = machine_context.get("machine_details", {})
            recent_maintenance = machine_context.get("recent_maintenance", [])
            recent_parts_usage = machine_context.get("recent_parts_usage", [])
            hours_trend = machine_context.get("hours_trend", [])
            maintenance_suggestions = machine_context.get("maintenance_suggestions", [])
            
            machine_info_prompts = {
                "en": f"""

MACHINE CONTEXT:
- Model: {machine_details.get('model_type', 'Unknown')}
- Serial Number: {machine_details.get('serial_number', 'Unknown')}
- Current Hours: {machine_details.get('current_hours', 0)}
- Organization: {machine_details.get('organization', 'Unknown')}
- Country: {machine_details.get('country', 'Unknown')}

RECENT MAINTENANCE HISTORY:
{self._format_maintenance_history(recent_maintenance, language)}

RECENT PARTS USAGE:
{self._format_parts_usage(recent_parts_usage, language)}

MACHINE HOURS TREND:
{self._format_hours_trend(hours_trend, language)}

MAINTENANCE RECOMMENDATIONS:
{self._format_maintenance_suggestions(maintenance_suggestions, language)}

Use this comprehensive machine information to provide highly targeted troubleshooting advice. Consider the machine's usage patterns, maintenance history, and any overdue maintenance when diagnosing problems.""",
                
                "el": f"""

Î Î›Î‘Î™Î£Î™ÎŸ ÎœÎ—Î§Î‘ÎÎ—ÎœÎ‘Î¤ÎŸÎ£:
- ÎœÎ¿Î½Ï„Î­Î»Î¿: {machine_details.get('model_type', 'Î†Î³Î½Ï‰ÏƒÏ„Î¿')}
- Î£ÎµÎ¹ÏÎ¹Î±ÎºÏŒÏ‚ Î‘ÏÎ¹Î¸Î¼ÏŒÏ‚: {machine_details.get('serial_number', 'Î†Î³Î½Ï‰ÏƒÏ„Î¿Ï‚')}
- Î¤ÏÎ­Ï‡Î¿Ï…ÏƒÎµÏ‚ ÎÏÎµÏ‚: {machine_details.get('current_hours', 0)}
- ÎŸÏÎ³Î±Î½Î¹ÏƒÎ¼ÏŒÏ‚: {machine_details.get('organization', 'Î†Î³Î½Ï‰ÏƒÏ„Î¿Ï‚')}
- Î§ÏŽÏÎ±: {machine_details.get('country', 'Î†Î³Î½Ï‰ÏƒÏ„Î·')}

Î Î¡ÎŒÎ£Î¦Î‘Î¤ÎŸ Î™Î£Î¤ÎŸÎ¡Î™ÎšÎŸ Î£Î¥ÎÎ¤Î—Î¡Î—Î£Î—Î£:
{self._format_maintenance_history(recent_maintenance, language)}

Î Î¡ÎŸÎ£Î¦Î‘Î¤Î— Î§Î¡Î—Î£Î— Î‘ÎÎ¤Î‘Î›Î›Î‘ÎšÎ¤Î™ÎšÎ©Î:
{self._format_parts_usage(recent_parts_usage, language)}

Î¤Î‘Î£Î— Î©Î¡Î©Î ÎœÎ—Î§Î‘ÎÎ—ÎœÎ‘Î¤ÎŸÎ£:
{self._format_hours_trend(hours_trend, language)}

Î£Î¥Î£Î¤Î‘Î£Î•Î™Î£ Î£Î¥ÎÎ¤Î—Î¡Î—Î£Î—Î£:
{self._format_maintenance_suggestions(maintenance_suggestions, language)}

Î§ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î®ÏƒÏ„Îµ Î±Ï…Ï„Î­Ï‚ Ï„Î¹Ï‚ Ï€ÎµÏÎ¹ÎµÎºÏ„Î¹ÎºÎ­Ï‚ Ï€Î»Î·ÏÎ¿Ï†Î¿ÏÎ¯ÎµÏ‚ Î¼Î·Ï‡Î±Î½Î®Î¼Î±Ï„Î¿Ï‚ Î³Î¹Î± Î½Î± Ï€Î±ÏÎ­Ï‡ÎµÏ„Îµ ÎµÎ¾Î±Î¹ÏÎµÏ„Î¹ÎºÎ¬ ÏƒÏ„Î¿Ï‡ÎµÏ…Î¼Î­Î½ÎµÏ‚ ÏƒÏ…Î¼Î²Î¿Ï…Î»Î­Ï‚ Î±Î½Ï„Î¹Î¼ÎµÏ„ÏŽÏ€Î¹ÏƒÎ·Ï‚ Ï€ÏÎ¿Î²Î»Î·Î¼Î¬Ï„Ï‰Î½.""",
                
                "ar": f"""

Ø³ÙŠØ§Ù‚ Ø§Ù„Ø¢Ù„Ø©:
- Ø§Ù„Ø·Ø±Ø§Ø²: {machine_details.get('model_type', 'ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ')}
- Ø§Ù„Ø±Ù‚Ù… Ø§Ù„ØªØ³Ù„Ø³Ù„ÙŠ: {machine_details.get('serial_number', 'ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ')}
- Ø§Ù„Ø³Ø§Ø¹Ø§Øª Ø§Ù„Ø­Ø§Ù„ÙŠØ©: {machine_details.get('current_hours', 0)}
- Ø§Ù„Ù…Ù†Ø¸Ù…Ø©: {machine_details.get('organization', 'ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ')}
- Ø§Ù„Ø¨Ù„Ø¯: {machine_details.get('country', 'ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ')}

ØªØ§Ø±ÙŠØ® Ø§Ù„ØµÙŠØ§Ù†Ø© Ø§Ù„Ø­Ø¯ÙŠØ«:
{self._format_maintenance_history(recent_maintenance, language)}

Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù‚Ø·Ø¹ Ø§Ù„Ø­Ø¯ÙŠØ«:
{self._format_parts_usage(recent_parts_usage, language)}

Ø§ØªØ¬Ø§Ù‡ Ø³Ø§Ø¹Ø§Øª Ø§Ù„Ø¢Ù„Ø©:
{self._format_hours_trend(hours_trend, language)}

ØªÙˆØµÙŠØ§Øª Ø§Ù„ØµÙŠØ§Ù†Ø©:
{self._format_maintenance_suggestions(maintenance_suggestions, language)}

Ø§Ø³ØªØ®Ø¯Ù… Ù‡Ø°Ù‡ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø´Ø§Ù…Ù„Ø© Ù„Ù„Ø¢Ù„Ø© Ù„ØªÙ‚Ø¯ÙŠÙ… Ù†ØµØ§Ø¦Ø­ Ø§Ø³ØªÙƒØ´Ø§Ù Ø£Ø®Ø·Ø§Ø¡ Ù…Ø³ØªÙ‡Ø¯ÙØ© Ù„Ù„ØºØ§ÙŠØ©.""",
                
                "es": f"""

CONTEXTO DE LA MÃQUINA:
- Modelo: {machine_details.get('model_type', 'Desconocido')}
- NÃºmero de Serie: {machine_details.get('serial_number', 'Desconocido')}
- Horas Actuales: {machine_details.get('current_hours', 0)}
- OrganizaciÃ³n: {machine_details.get('organization', 'Desconocida')}
- PaÃ­s: {machine_details.get('country', 'Desconocido')}

HISTORIAL DE MANTENIMIENTO RECIENTE:
{self._format_maintenance_history(recent_maintenance, language)}

USO RECIENTE DE PIEZAS:
{self._format_parts_usage(recent_parts_usage, language)}

TENDENCIA DE HORAS DE MÃQUINA:
{self._format_hours_trend(hours_trend, language)}

RECOMENDACIONES DE MANTENIMIENTO:
{self._format_maintenance_suggestions(maintenance_suggestions, language)}

Usa esta informaciÃ³n integral de la mÃ¡quina para proporcionar consejos de soluciÃ³n de problemas altamente dirigidos.""",
                
                "tr": f"""

MAKÄ°NE BAÄžLAMI:
- Model: {machine_details.get('model_type', 'Bilinmiyor')}
- Seri NumarasÄ±: {machine_details.get('serial_number', 'Bilinmiyor')}
- Mevcut Saatler: {machine_details.get('current_hours', 0)}
- Organizasyon: {machine_details.get('organization', 'Bilinmiyor')}
- Ãœlke: {machine_details.get('country', 'Bilinmiyor')}

SON BAKIM GEÃ‡MÄ°ÅžÄ°:
{self._format_maintenance_history(recent_maintenance, language)}

SON PARÃ‡A KULLANIMI:
{self._format_parts_usage(recent_parts_usage, language)}

MAKÄ°NE SAATLERÄ° EÄžÄ°LÄ°MÄ°:
{self._format_hours_trend(hours_trend, language)}

BAKIM Ã–NERÄ°LERÄ°:
{self._format_maintenance_suggestions(maintenance_suggestions, language)}

Son derece hedefli sorun giderme tavsiyeleri saÄŸlamak iÃ§in bu kapsamlÄ± makine bilgilerini kullanÄ±n.""",
                
                "no": f"""

MASKINKONTEKST:
- Modell: {machine_details.get('model_type', 'Ukjent')}
- Serienummer: {machine_details.get('serial_number', 'Ukjent')}
- NÃ¥vÃ¦rende timer: {machine_details.get('current_hours', 0)}
- Organisasjon: {machine_details.get('organization', 'Ukjent')}
- Land: {machine_details.get('country', 'Ukjent')}

NYLIG VEDLIKEHOLDSHISTORIKK:
{self._format_maintenance_history(recent_maintenance, language)}

NYLIG DELEBRUK:
{self._format_parts_usage(recent_parts_usage, language)}

MASKINTIMERTREND:
{self._format_hours_trend(hours_trend, language)}

VEDLIKEHOLDSANBEFALINGER:
{self._format_maintenance_suggestions(maintenance_suggestions, language)}

Bruk denne omfattende maskininformasjonen til Ã¥ gi svÃ¦rt mÃ¥lrettet feilsÃ¸kingsrÃ¥d."""
            }
            
            machine_info = machine_info_prompts.get(language, machine_info_prompts["en"])
            base_prompt += machine_info
        
        return base_prompt
    
    def _format_maintenance_history(self, maintenance_history: List[Dict[str, Any]], language: str) -> str:
        """Format maintenance history for display in prompts."""
        if not maintenance_history:
            return "No recent maintenance records available."
        
        formatted_records = []
        for record in maintenance_history:
            date = record.get("date", "Unknown")
            maintenance_type = record.get("type", "Unknown")
            description = record.get("description", "No description")
            parts_used = record.get("parts_used", 0)
            performed_by = record.get("performed_by", "Unknown")
            
            formatted_records.append(f"- {date}: {maintenance_type} - {description} ({parts_used} parts used, by {performed_by})")
        
        return "\n".join(formatted_records)
    
    def _format_parts_usage(self, parts_usage: List[Dict[str, Any]], language: str) -> str:
        """Format parts usage for display in prompts."""
        if not parts_usage:
            return "No recent parts usage records available."
        
        formatted_usage = []
        for usage in parts_usage:
            date = usage.get("date", "Unknown")
            part_name = usage.get("part_name", "Unknown part")
            part_number = usage.get("part_number", "Unknown")
            quantity = usage.get("quantity", 0)
            is_proprietary = usage.get("is_proprietary", False)
            proprietary_note = " (Proprietary)" if is_proprietary else ""
            
            formatted_usage.append(f"- {date}: {part_name} ({part_number}){proprietary_note} - Qty: {quantity}")
        
        return "\n".join(formatted_usage)
    
    def _format_hours_trend(self, hours_trend: List[Dict[str, Any]], language: str) -> str:
        """Format machine hours trend for display in prompts."""
        if not hours_trend:
            return "No recent hours records available."
        
        formatted_hours = []
        for record in hours_trend:
            date = record.get("date", "Unknown")
            hours = record.get("hours", 0)
            recorded_by = record.get("recorded_by", "Unknown")
            
            formatted_hours.append(f"- {date}: {hours} hours (recorded by {recorded_by})")
        
        return "\n".join(formatted_hours)
    
    def _format_maintenance_suggestions(self, suggestions: List[Dict[str, Any]], language: str) -> str:
        """Format maintenance suggestions for display in prompts."""
        if not suggestions:
            return "No maintenance recommendations at this time."
        
        formatted_suggestions = []
        for suggestion in suggestions:
            suggestion_type = suggestion.get("type", "Unknown")
            description = suggestion.get("description", "No description")
            priority = suggestion.get("priority", "medium")
            overdue_hours = suggestion.get("overdue_hours", 0)
            
            priority_indicator = "ðŸ”´" if priority == "high" else "ðŸŸ¡" if priority == "medium" else "ðŸŸ¢"
            overdue_note = f" (Overdue by {overdue_hours} hours)" if overdue_hours > 0 else ""
            
            formatted_suggestions.append(f"- {priority_indicator} {suggestion_type}: {description}{overdue_note}")
        
        return "\n".join(formatted_suggestions)
    
    async def generate_simple_response(
        self,
        prompt: str,
        context: Dict[str, Any],
        language: str = "en"
    ) -> str:
        """
        Simple generate_response method for compatibility with sessions router.
        
        Args:
            prompt: User's message/prompt
            context: Context information including conversation history
            language: Target language for response
            
        Returns:
            Generated response content
        """
        # If client is not initialized, return a mock response
        if not self.client:
            return "AI assistant unavailable. Check OpenAI API configuration."
        
        # Convert context to messages format
        messages = []
        
        # Add system message
        system_msg = f"You are AutoBoss AI Assistant, helping users troubleshoot AutoBoss net cleaning machines. Respond in {language}."
        messages.append(ConversationMessage(role="system", content=system_msg))
        
        # Add conversation history if available
        if "conversation_history" in context:
            for msg in context["conversation_history"]:
                messages.append(ConversationMessage(
                    role=msg["sender"] if msg["sender"] != "assistant" else "assistant",
                    content=msg["content"]
                ))
        
        # Add current user message
        messages.append(ConversationMessage(role="user", content=prompt))
        
        # Generate response using the full method
        response = await self.generate_response(
            messages=messages,
            language=language
        )
        
        return response.content if response.success else (response.error_message or "Could not generate response.")
    
    async def generate_embedding(self, text: str, model: str = "text-embedding-ada-002") -> List[float]:
        """
        Generate embedding vector for text using OpenAI embeddings API.
        
        Args:
            text: Text to generate embedding for
            model: Embedding model to use
            
        Returns:
            List of floats representing the embedding vector
        """
        if not self.client:
            logger.error("OpenAI client not initialized for embedding generation")
            # Return a zero vector of the expected dimension (1536 for ada-002)
            return [0.0] * 1536
        
        try:
            # Clean and truncate text if necessary
            text = text.strip()
            if len(text) > 8000:  # OpenAI embedding limit is ~8191 tokens
                text = text[:8000]
            
            if not text:
                logger.warning("Empty text provided for embedding generation")
                return [0.0] * 1536
            
            response = await self.client.embeddings.create(
                model=model,
                input=text
            )
            
            embedding = response.data[0].embedding
            logger.debug(f"Generated embedding of dimension {len(embedding)} for text length {len(text)}")
            
            return embedding
            
        except openai.RateLimitError as e:
            logger.warning(f"Rate limit hit during embedding generation: {e}")
            # Wait and retry once
            await asyncio.sleep(1)
            try:
                response = await self.client.embeddings.create(
                    model=model,
                    input=text
                )
                return response.data[0].embedding
            except Exception as retry_e:
                logger.error(f"Failed to generate embedding after retry: {retry_e}")
                return [0.0] * 1536
                
        except Exception as e:
            logger.error(f"Failed to generate embedding: {e}")
            return [0.0] * 1536